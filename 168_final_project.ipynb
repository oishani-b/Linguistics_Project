{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4361e14f-5b7a-4312-a3ae-46563893915e",
   "metadata": {},
   "source": [
    "# Experimental Project\n",
    "## Measuring Error for ASR Systems Between Certain Dialects or Accents in English (specifically Indian dialects vs Southern California dialects)\n",
    "\n",
    "- ##### What tool or tools did you experiment on? \n",
    "I used the Speech-to-Text APIs to measure the performance of 2 ASR systems in particular: the\n",
    "Whisper API and the Microsoft Azure AI REST API.\n",
    "\n",
    "- ##### How did you interact with the tool? \n",
    "I installed these 2 APIs locally and called them in Python. All of my code is in Python, formatted\n",
    "in Jupyter Notebooks with Markdown interspersed to explain the code chunks. For the most part, I used\n",
    "libraries in Python and created functions for specific string comparisons. For this project, I focused\n",
    "on individual strings as sentences for the input, since iterating through a csv made it more complicated\n",
    "and the results were harder to keep track of. I have written a Python file that uses these APIs and\n",
    "returns the transcripts from the audio files into individual text files as well as a CSV.\n",
    "\n",
    "My code for this transcription process can be found at my __[GitHub](https://github.com/oishani-b/Linguistics_Project.git)__ here. This transcription code should work on any computer or device as long as the required\n",
    "packages and libraries have been locally installed. Currently, it works specifically for '.ogg' and '.wav'\n",
    "files. You can try out this code with your own directory of audio files by changing the file paths\n",
    "to your localy directories! Although this is a very important aspect of this project, my goal with the LIGN 168 \n",
    "Final Project is to refine my error measurement methods, and\n",
    "compare word-wise error measurements vs phonemic error measurements.\n",
    "\n",
    "Since I collected an expandive aray of recordings, and I wanted to refine specifically my method of capturing error,\n",
    "I used a specific subsets of the transcripts to make comparisons with the target sentence.\n",
    "\n",
    "\n",
    "- ##### What hypotheses did you test? \n",
    "My general hypothesis is that these ASR systems will have significantly higher error rates\n",
    "with Indian English dialects than California English dialects. Also, I think that the\n",
    "most effective error measurement will be at the word level, since syllable level might\n",
    "be so granular that it causes universally high error rates, whereas phrase or\n",
    "sentence level may cause intuitively wrong similar error rates.\n",
    "  \n",
    "- ##### How did you test them?\n",
    "I used a multi-step process, both at the word-level and the phoneme-level, to test this hypothesis. \n",
    "\n",
    "To measure error at the word level, I turned each sentence or string into a list of words, and then compared the word lists from the target sentences (the prompts shown to participants) to the word lists produced by each of the transcription services. By eliminating all of the words that the transcriptions got right, I was left with a list of the words that they did not get, subsituted, or added. \n",
    "\n",
    "Next, I used the word lists from above to compare the general pronunciations of these words using CMUdict. By using the arpabet transcriptions of each of the erroneous parts of the sentences, I could gain some idea of which phonemes had been missed, substituted, or added. This was more challenging than expected, due to the wide variety of possible errors made by these systems. But, I found that a similar process of adding all the phonemes to a list and then eliminating based on commonalities between the target and transcript lists, worked quite well for my example cases. This is something I might refine further depending on the different complexities of errors I see in these systems. Since I'm working with a relatively limited subset of the transcriptions to make sure I have good base cases, there is always the possibility of edge cases that I have not successfully addressed.\n",
    "\n",
    "\n",
    "- ##### How do you know you’ve answered this question satisfactorily? (Or, if you’ve found that you can’t answer the question, explain why and engage in a nuanced way with what would need to be different to answer it in the future?)\n",
    "\n",
    "I think I answered this question satisfactorily because I could observe the differences in each of my error measurements clearly, and trace them back using my code. I made sure to add several print statements and Markdown explanations throughout my code to make it explainable at every step of the process. \n",
    "\n",
    "Perhaps the most interesting results were Microsoft Azure's relatively great transcriptions compared to Whisper with the Indian dialects! Also, I found some noticeable differences across dialects in terms of numbers of errors, both word-wise and phoneme-wise. This gives me an idea of the possible statistically significant differences across speakers of Southern California vs Indian dialects. I also found some interesting results depending on style of speaking and background noise, explained further under the next question. \n",
    "\n",
    "Generally, I found that the Southern California dialect had higher accuracy than the 2 Indian dialects I tested, but this was especially noticeable with Whisper. The Azure system did either marginally worse for one of the Indian dialects or equally well for all 3 dialects. The ASR systems did surprisingly well with some of the prompts I thought would be heavily influenced by dialect, and universally poorly on some of the prompts I thought would be perceived more accurately in a Southern California dialect. \n",
    "\n",
    "While I thought word-error measurements seemed more intuitively correct in terms of eliminations, substitutions and deletions of words, I was surprised to find that the phonemic-error measurements were actually very intuitive and insightful. The fine-grained knowledge of eactly where the ASR systems were transcribing different vowels, for example, was very useful. While the word-level errors do make some sense in terms of human intuitions for what you would consider an error in recognizing speech, the phoneme-level errors are generally more intuitive and accurate in terms of measuring errors, especially for substitutions with similar pronunciatinos that are not well-captured by the broad strokes of word-level measurment.\n",
    "\n",
    "- ##### What challenges did you face?\n",
    "An unexpected factor that threw me off was the difference made by variance in speech styles. I used one participant's recordings, who used a more casual speech style with an Indian dialect (specifically Tamil), and added my own recordings with a more formal speech style, and it made a noticeable difference in the accuracy of the transcriptions. \n",
    "\n",
    "I was suprised to find the relatively high levels of accuracy when speakers used formal speech styles in general, regardless of dialect. I will definitely keep this factor in mind when recording participants in the future, because casual speech style was changing the transcripts to be far more erronious. \n",
    "\n",
    "I re-recorded a few prompts with participants and compared data across a variation of factors, such as speech style, closeness to the microphone, audio quality (based on the device thy were using), and background noise. I found that any loud background noise simply did not work, since the ASR systems would begin to transrribe voices from the background. \n",
    "\n",
    "To keep my focus on error measurement, I selected tramscripts that had some variation, but not much. For example, my transcripts are generally in a more formal style of speech and with minimal background noise. Some of the Southern California dialect tramscripts have some background noise or casual speech style, as do some of the other Indian dialect transcripts. Generally, though, I chose some of the relatively simple errors to investigate, so that I could successfully build base cases and some usefuledge cases to test my error measurement methods.\n",
    "\n",
    "- ##### If your study involved human participants, how were they consented?\n",
    "I asked my participants if they would agree to a UCSD Informed\n",
    "Consent form (the template was taken from the IRB at UCSD). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d54f5-fb82-490f-9359-5034282971fc",
   "metadata": {},
   "source": [
    "# Word Level Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0e7dc52-1883-47a0-8f5a-0313cd34513d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter warnings so that they only appear once\n",
    "import warnings \n",
    "warnings.filterwarnings(action='once')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcba0cc-1add-49b8-9588-b082dfc6b825",
   "metadata": {},
   "source": [
    "### Defining Functions for Word Error Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc495f3-1200-4b1f-8ad6-07c7b280a9cc",
   "metadata": {},
   "source": [
    "The first step in the process of error measurment was to map out differences between the target string and the transcript string at the word level. To do this, I followed a multi-step process to ensure I was preserving as much of the original data as possible, and meaningfully drawing out steps to extract the transcriptions.\n",
    "\n",
    "The steps I used were:\n",
    "1) Convert the sentences into lists of words\n",
    "2) Remove punctuation and capitalization for easier comparisons\n",
    "3) Make the prompt and transcript word lisst equal in length for iteration\n",
    "4) By iterating through the prompt list, eliminate all words the transcript got right, leaving only the problem words\n",
    "5) Count the number of errors using the problem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d52c5d0-14a0-4aac-bede-0770edb5be89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the input sentence into a list of words\n",
    "def to_list(sentence):\n",
    "    word_list = sentence.split()\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa72f8-afb5-4e27-a435-0a51bd779ef3",
   "metadata": {},
   "source": [
    "Next, I decided to strip the punctuation from the split words, so that the target sentence's words could be compared as accurately as possible to the transcript's words, regardless of surrounding punctuation. I also change all the words to lowercase, again to stay consistent throughout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86f7b8ca-5be3-4014-bba7-39e65309c2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:3: SyntaxWarning: invalid escape sequence '\\,'\n"
     ]
    }
   ],
   "source": [
    "#remove punctuation from each of the words in the word list\n",
    "def depunctuate(word_list):\n",
    "    punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "    depunct_word_list = []\n",
    "    for i in word_list: \n",
    "        #iterate through each character in the word\n",
    "        for character in i:\n",
    "            #checking if the character is a punctuation mark and replacing it with an empty string\n",
    "            if character in punc:     \n",
    "                i = i.replace(character, \"\")\n",
    "        #adding the new word without punctuation in lowercase to the result list\n",
    "        depunct_word_list.append(i.lower())\n",
    "    return depunct_word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516d95e4-d956-45ac-aacb-4dd3ea17903b",
   "metadata": {},
   "source": [
    "This function gave me each of the word lists, so I was ready to start comparing the lists.\n",
    "\n",
    "Here, I ran into an issue with indexing. Either one of the lists being longer meant it was hard to iterate through values in them. So, I decided to make both lists the same length by adding empty strings to whichever word list was shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c9aac03-567e-4442-92cc-9e51a613a7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make the shorter list equal in length to the longer one\n",
    "def equalize_list_lengths(targ_list, err_list):\n",
    "    \n",
    "    #check if the transcript word list has more words\n",
    "    if len(err_list) > len(targ_list):\n",
    "        extra_errors = len(err_list) - len(targ_list)\n",
    "        #add empty strings as dummy words now, depending on how many extra words the transcript has\n",
    "        for i in range(0, extra_errors):\n",
    "            targ_list.append(\"\")\n",
    "\n",
    "    #check if the prompt word list has more words\n",
    "    elif len(targ_list) > len(err_list):\n",
    "        targ_missed = len(targ_list) - len(err_list)\n",
    "        #add empty strings as dummy words now, depending on how many missed words the prompt has\n",
    "        for i in range(0, targ_missed):\n",
    "            err_list.append(\"\")\n",
    "            \n",
    "    #return a list containing 2 lists of equal length: the prompt word list and target word list with extra empty strings\n",
    "    return [targ_list, err_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383090a3-3a58-4866-8ef8-1982d0d53ff3",
   "metadata": {},
   "source": [
    "After making the lists equal in length, I started capturing the missing, substituted, and added words in the transcript. I did this by checking off each of the words in the prompt word list with the transcript word list.\n",
    "\n",
    "I tried a few different ways to write this function, since it was important to get the 'last words standing' correct. Directly changing the prompt word list was leading to issues, so I initiated a new list for the words in the prompt that were missing in the transcript. I only removed the overlapping words from the transcript word list. I also used this function to remove the extra empty string 'dummy words' we had added in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5cb65637-d182-4643-8fd2-d7311a33e1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_words(ready_targ_list, ready_err_list):\n",
    "\n",
    "    #this list will store all the words present in the prompt, but missing from the transcript\n",
    "    targ_leftovers = []\n",
    "    \n",
    "    for i in ready_targ_list:\n",
    "        \n",
    "        #if the word is found in both the prompt and the transcript, it is correctly captured        \n",
    "        if (i in ready_err_list and i in ready_targ_list):\n",
    "            # the correctly captured word is removed from the transcript word list\n",
    "            ready_err_list.remove(i)\n",
    "            \n",
    "        #if the word is not found in both the prompt and the transcript, something must be wrong\n",
    "        else:\n",
    "            #the word from the prompt that is missing from the transcript is added to this list\n",
    "            targ_leftovers += [i]\n",
    "\n",
    "    #this removes the empty strings, leaving a list of only the words present in the prompt, but \n",
    "    #missing from the transcript\n",
    "    ready_targ_list = []\n",
    "    for i in targ_leftovers:\n",
    "        if len(i) != 0:\n",
    "            ready_targ_list.append(i)\n",
    "\n",
    "    #this list will store all the non-empty words remaining in the transcript list\n",
    "    #these words were not in the prompt, so they are either substitutions or additions\n",
    "    temp_err_list = []\n",
    "    for i in ready_err_list:\n",
    "        if len(i) != 0:\n",
    "            temp_err_list.append(i)\n",
    "    ready_err_list = temp_err_list\n",
    "\n",
    "    return [ready_targ_list, ready_err_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6d945d-e033-4130-9ffb-4e07b0d00c62",
   "metadata": {},
   "source": [
    "At this point, I came to a tricky question: how do I actually count the errors? I have a list of words that were missed, and a list of words that were not in the prompt, but either of those could be substitutions.\n",
    "\n",
    "I think there may be other ways to do this calculation, but I generally found that the longer list of words was a good metric. It typically accounted for substitutions without double-counting, and also captured omissions from the smaller list. This is an area I would like to explore and refine further, but I think it worked well for my example transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2d5e18-1416-42ae-bb21-815b6fd27e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of errors between the word lists\n",
    "def counter(final_targ_list, final_err_list):\n",
    "    if len(final_targ_list) == len(final_err_list):\n",
    "        return len(final_targ_list)\n",
    "    elif len(final_targ_list) > len(final_err_list):\n",
    "        return len(final_targ_list)\n",
    "    elif len(final_targ_list) < len(final_err_list):\n",
    "        return len(final_err_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a891bdde-b364-451d-8c8b-124d7e1390e5",
   "metadata": {},
   "source": [
    "After completing these steps, I had a good metric for calculating word-level error rates. I compiled all the functions above into one function so that it's easier to run and follows the right sequence.\n",
    "\n",
    "The input for this function is 2 strings: the target sentence and the transcription sentence.\n",
    "\n",
    "The output is a list with 3 values: 1) the prompt words that were missing, 2) any substituitons or additions, and 3) the number of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "803dd460-99b5-4524-b1d9-676bced8dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_finder(target_sentence, error_sentence):\n",
    "    #step 1\n",
    "    targ_list = to_list(target_sentence)\n",
    "    err_list = to_list(error_sentence)\n",
    "\n",
    "    #step 2\n",
    "    depunct_targ_list = depunctuate(targ_list)\n",
    "    depunct_err_list = depunctuate(err_list)\n",
    "\n",
    "    #step 3\n",
    "    eq_length_lists = equalize_list_lengths(depunct_targ_list, depunct_err_list)\n",
    "    ready_targ_list = eq_length_lists[0]\n",
    "    ready_err_list = eq_length_lists[1]\n",
    "\n",
    "    #step 4\n",
    "    final_error_lists = get_error_words(ready_targ_list, ready_err_list)\n",
    "    final_targ_list = final_error_lists[0]\n",
    "    print(\"List of words missing from prompt in transcription: \", final_targ_list)\n",
    "    final_err_list = final_error_lists[1]\n",
    "    print(\"List of words in transcription not present in prompt: \", final_err_list)\n",
    "\n",
    "    #step 5\n",
    "    final_err_count = counter(final_targ_list, final_err_list)\n",
    "\n",
    "    return [final_targ_list, final_err_list, final_err_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb47c1d-ff30-4a13-bb98-44603de24588",
   "metadata": {},
   "source": [
    "### Testing Word Error Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978e52c2-9d98-45c0-b1d0-65b4a60ed9f8",
   "metadata": {},
   "source": [
    "Next, I tested this function using some of the results from the transctiptions I had. The first example I used is from an older set of transcriptions: I added some possible transcriptions as examples of different cases. The next few examples are from the results of the transcriptions I completed for this class project specifically. I used a limited number of examples, but had some variation throughout them to check if my general implementation was working as expected. The tests are below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d703ba94-f5aa-4b75-9080-4a3d966c2638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript 1\n",
      "List of words missing from prompt in transcription:  ['bangalore']\n",
      "List of words in transcription not present in prompt:  ['bong', 'load']\n",
      "[['bangalore'], ['bong', 'load'], 2]\n",
      "\n",
      "Transcript 2\n",
      "List of words missing from prompt in transcription:  ['for', 'bangalore']\n",
      "List of words in transcription not present in prompt:  []\n",
      "[['for', 'bangalore'], [], 2]\n",
      "\n",
      "Transcript 3\n",
      "List of words missing from prompt in transcription:  ['leaving']\n",
      "List of words in transcription not present in prompt:  ['leafing', 'then']\n",
      "[['leaving'], ['leafing', 'then'], 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_1 = \"She was leaving for Bangalore that day.\"\n",
    "error_sentence_1 = \"She was leaving for bong load that day.\"\n",
    "error_sentence_2 = \"She was leaving that day.\"\n",
    "error_sentence_3 = \"She was leafing for Bangalore that day then.\"\n",
    "\n",
    "print(\"Transcript 1\")\n",
    "print(error_finder(target_sentence_1, error_sentence_1))\n",
    "print()\n",
    "\n",
    "print(\"Transcript 2\")\n",
    "print(error_finder(target_sentence_1, error_sentence_2))\n",
    "print()\n",
    "\n",
    "print(\"Transcript 3\")\n",
    "print(error_finder(target_sentence_1, error_sentence_3))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac8b268d-4f71-41f0-995c-11ce427ffda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You haven't even been to the In-n-Out in the Outback Steakhouse neighborhood?\n",
      "\n",
      "socal_1_whisper\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['end', 'and', 'out']\n",
      "[['innout'], ['end', 'and', 'out'], 3]\n",
      "\n",
      "indian_1_whisper\n",
      "List of words missing from prompt in transcription:  ['innout', 'steakhouse', 'neighborhood']\n",
      "List of words in transcription not present in prompt:  ['inn', 'and', 'out', 'stack', 'or', 'sniper', 'herd']\n",
      "[['innout', 'steakhouse', 'neighborhood'], ['inn', 'and', 'out', 'stack', 'or', 'sniper', 'herd'], 7]\n",
      "\n",
      "indian_2_whisper\n",
      "List of words missing from prompt in transcription:  ['you', 'innout']\n",
      "List of words in transcription not present in prompt:  ['youll', 'inn', 'and', 'out']\n",
      "[['you', 'innout'], ['youll', 'inn', 'and', 'out'], 4]\n",
      "\n",
      "socal_1_azure\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['and', 'out', 'in']\n",
      "[['innout'], ['and', 'out', 'in'], 3]\n",
      "\n",
      "indian_1_azure\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['and', 'out', 'in']\n",
      "[['innout'], ['and', 'out', 'in'], 3]\n",
      "\n",
      "indian_2_azure\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['and', 'out', 'in']\n",
      "[['innout'], ['and', 'out', 'in'], 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_2 = \"You haven't even been to the In-n-Out in the Outback Steakhouse neighborhood?\"\n",
    "print(target_sentence_2)\n",
    "print()\n",
    "\n",
    "socal_1_whisper = \"You haven't even been to the end and out in the Outback Steakhouse neighborhood.\"\n",
    "indian_1_whisper = \"You haven't even been to the inn and out in the outback stack or sniper herd.\"\n",
    "indian_2_whisper = \"You'll haven't even been to the inn and out in the Outback Steakhouse neighborhood.\"\n",
    "\n",
    "socal_1_azure = \"you haven't even been to the in and out in the outback steakhouse neighborhood\"\n",
    "indian_1_azure = \"you haven't even been to the in and out in the outback steakhouse neighborhood\"\n",
    "indian_2_azure = \"you haven't even been to the in and out in the outback steakhouse neighborhood\"\n",
    "\n",
    "print(\"socal_1_whisper\")\n",
    "print(error_finder(target_sentence_2, socal_1_whisper))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_whisper\")\n",
    "print(error_finder(target_sentence_2, indian_1_whisper))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_whisper\")\n",
    "print(error_finder(target_sentence_2, indian_2_whisper))\n",
    "print()\n",
    "\n",
    "print(\"socal_1_azure\")\n",
    "print(error_finder(target_sentence_2, socal_1_azure))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_azure\")\n",
    "print(error_finder(target_sentence_2, indian_1_azure))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_azure\")\n",
    "print(error_finder(target_sentence_2, indian_2_azure))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571ce1fa-9059-42ce-bf52-27172e131fbd",
   "metadata": {},
   "source": [
    "These results are pretty interesting! Azure did a much better job than Whisper overall. Whisper recognized more errors for the 2 Indian dialects than for the single Southern California dialect. \n",
    "\n",
    "Also, these results give us an intuitive understanding of the errors fairly easily.\n",
    "\n",
    "As an example, the 7 errors captured in the 'indian_1_whisper' dialect show steakhouse being replaced by steak, or being added, neighborhood being replaced by sniper and herd, and In-n-Out being replaced by inn and out. These errors make sense as intuitive measurements, but there are some gray areas here.\n",
    "\n",
    "For example, In-n-Out (which looks like innout after the punctuation removal, but looked like In-n-Out in the prompt, seems to get 'in and out' the most often, even though it has a distinct pronunciation. We could count this as a single error, ubt it also makes sense to count it as 3 separate errors because of how In-n-Out is typically pronounced. In this case, a phoneme-level analysis would be very useful!\n",
    "\n",
    "Here's another set of results that I analyzed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "150f9ac6-7d65-4df6-857f-8b628aea293c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro sure with all my work I've made it to the brochure.\n",
      "\n",
      "socal_1_whisper\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure']\n",
      "List of words in transcription not present in prompt:  ['brochure']\n",
      "[['bro', 'sure'], ['brochure'], 2]\n",
      "\n",
      "indian_1_whisper\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure', 'with', 'it', 'brochure']\n",
      "List of words in transcription not present in prompt:  ['im', 'going', 'show', 'you', 'to', 'brush']\n",
      "[['bro', 'sure', 'with', 'it', 'brochure'], ['im', 'going', 'show', 'you', 'to', 'brush'], 6]\n",
      "\n",
      "indian_2_whisper\n",
      "List of words missing from prompt in transcription:  ['sure']\n",
      "List of words in transcription not present in prompt:  ['share']\n",
      "[['sure'], ['share'], 1]\n",
      "\n",
      "socal_1_azure\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure']\n",
      "List of words in transcription not present in prompt:  ['brochure']\n",
      "[['bro', 'sure'], ['brochure'], 2]\n",
      "\n",
      "indian_1_azure\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure']\n",
      "List of words in transcription not present in prompt:  ['share']\n",
      "[['bro', 'sure'], ['share'], 2]\n",
      "\n",
      "indian_2_azure\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "[[], [], 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_3 = \"Bro sure with all my work I've made it to the brochure.\"\n",
    "print(target_sentence_3)\n",
    "print()\n",
    "\n",
    "indian_2_whisper = \"Bro, share with all my work. I've made it to the brochure.\"\n",
    "socal_1_whisper = \"brochure with all my work I've made it to the brochure.\"\n",
    "indian_1_whisper = \"I'm going to show you all my work I've made to the brush.\"\n",
    "\n",
    "indian_2_azure = \"bro sure with all my work i've made it to the brochure\"\n",
    "socal_1_azure = \"brochure with all my work i've made it to the brochure\"\n",
    "indian_1_azure = \"share with all my work i've made it to the brochure\"\n",
    "\n",
    "print(\"socal_1_whisper\")\n",
    "print(error_finder(target_sentence_3, socal_1_whisper))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_whisper\")\n",
    "print(error_finder(target_sentence_3, indian_1_whisper))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_whisper\")\n",
    "print(error_finder(target_sentence_3, indian_2_whisper))\n",
    "print()\n",
    "\n",
    "print(\"socal_1_azure\")\n",
    "print(error_finder(target_sentence_3, socal_1_azure))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_azure\")\n",
    "print(error_finder(target_sentence_3, indian_1_azure))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_azure\")\n",
    "print(error_finder(target_sentence_3, indian_2_azure))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb245e0-a463-4133-8fd1-0a123ae83ad9",
   "metadata": {},
   "source": [
    "Again, Azure does quite a bit better than Whisper, with one of the sentences exactly right! In this example, we can see where the measurement might not exactly reflect my intuitions: brochure as a subsitution for bro sure seems to make a lot more sense than share as a subsitution for bro sure. Here, again, phonemic analysis would be helpful!\n",
    "\n",
    "Generally, though, it seems like Azure is doing fairly well with the Indian dialects, whereas Whisper is struggling quite noticeably.\n",
    "\n",
    "This was also one of the cases where speech style played a role. In my transcription, I spoke formally and deliberatly, unlike the other 2 participants. This is reflected in the lowest error counts for both Whisper and Azure: leading to more insight about how much of a difference speech style can make in ASR interactions!\n",
    "\n",
    "Next, we tried testing an Indian pronunciation of an Indian place name in a slightly different example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d15eb2e-2736-4247-90f0-9bccbf8446c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is leaving for Bengaluru tomorrow.\n",
      "\n",
      "socal_1_whisper\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "[[], [], 0]\n",
      "\n",
      "indian_1_whisper\n",
      "List of words missing from prompt in transcription:  ['he', 'is', 'bengaluru']\n",
      "List of words in transcription not present in prompt:  ['hes', 'bangalore']\n",
      "[['he', 'is', 'bengaluru'], ['hes', 'bangalore'], 3]\n",
      "\n",
      "indian_2_whisper\n",
      "List of words missing from prompt in transcription:  ['bengaluru']\n",
      "List of words in transcription not present in prompt:  ['bangalore']\n",
      "[['bengaluru'], ['bangalore'], 1]\n",
      "\n",
      "socal_1_azure\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "[[], [], 0]\n",
      "\n",
      "indian_1_azure\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "[[], [], 0]\n",
      "\n",
      "indian_2_azure\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "[[], [], 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_4 = \"He is leaving for Bengaluru tomorrow.\"\n",
    "print(target_sentence_4)\n",
    "print()\n",
    "\n",
    "indian_1_whisper = \"He's leaving for Bangalore tomorrow\"\n",
    "indian_2_whisper = \"He is leaving for Bangalore tomorrow.\"\n",
    "socal_1_whisper = \"He is leaving for Bengaluru tomorrow.\"\n",
    "\n",
    "indian_1_azure = \"he is leaving for bengaluru tomorrow\"\n",
    "indian_2_azure = \"he is leaving for bengaluru tomorrow\"\n",
    "socal_1_azure = \"he is leaving for bengaluru tomorrow\"\n",
    "\n",
    "print(\"socal_1_whisper\")\n",
    "print(error_finder(target_sentence_4, socal_1_whisper))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_whisper\")\n",
    "print(error_finder(target_sentence_4, indian_1_whisper))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_whisper\")\n",
    "print(error_finder(target_sentence_4, indian_2_whisper))\n",
    "print()\n",
    "\n",
    "print(\"socal_1_azure\")\n",
    "print(error_finder(target_sentence_4, socal_1_azure))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_azure\")\n",
    "print(error_finder(target_sentence_4, indian_1_azure))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_azure\")\n",
    "print(error_finder(target_sentence_4, indian_2_azure))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ab30e4-5235-43f2-9030-844358712005",
   "metadata": {},
   "source": [
    "Azure absolutely nailed it this time! Honestly, I'm very impressed at Microsoft Azure's performance on this overall, and I think it's very interesting to learn about how each of these APIs has different error sensitivities to dialect. Obviously, these are not nearly enough examples to say that Azure is miles ahead or does not have any dialect/accent issues, but it's done a great job so far!\n",
    "\n",
    "Whisper, on the other hand, struggles with the Indian dialects again, this time in an even more interesting manner. The Southern California dialect saying 'Bengaluru' gets picked up, but the Indian dialects get picked up as Bangalore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82521a3-c672-4d3c-a7a9-d03ab2279a67",
   "metadata": {},
   "source": [
    "### Overall Word Error Measurement: Not Bad, but Missing Some Important Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee140f-5188-4459-b1f0-5f8ca9eb2052",
   "metadata": {},
   "source": [
    "From these tests, it's clear that Word-Level Error Measurements do give us meaningful insights into how the ASR systems are failing. I was especially surprised at the stark difference in performance across Azure and Whisper, with Azure outperforming Whisper in most of the example test cases I used. \n",
    "\n",
    "One of the caveats of word-level measurement if the lack of nuance seen in some of the transcription errors above. Approaching error measurement with phonemic analysis would help to handle these situations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24dc12-044b-452e-9da9-7355328a8680",
   "metadata": {},
   "source": [
    "# Phoneme Level Analysis Using CMUdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafe0da8-7377-4682-9fd4-6cabef6f37db",
   "metadata": {},
   "source": [
    "### Defining Functions for Phoneme Error Measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a1d43-14a3-48b5-b10c-0cec33858ad1",
   "metadata": {},
   "source": [
    "I used the word error lists created already to calculate the phoneme-level errors. I followed a step-by-step process similar to the word-level error measurement process, but incorporated CMUdict. \n",
    "\n",
    "The steps I used were:\n",
    "1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9d2810d-00ed-43ef-885b-a77d1ce6f872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the required libraries for cmudict\n",
    "import nltk\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "cmu_dict = cmudict.dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbfc606-ea9a-47bb-8a09-a31b28532742",
   "metadata": {},
   "source": [
    "After loading CMUdict and looking through it, I could not find Bengaluru or In-n-Out. So, I updated CMUdict with custom entries before using it for error mea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "842a438f-cc86-46a0-9d69-3d9cee8829f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add custom entries\n",
    "custom_entries = {\n",
    "    \"bengaluru\": [['B', 'EH', 'NG', 'G', 'UH', 'L', 'UW', 'R', 'UW']],\n",
    "    \"innout\": [['IH2', 'N', 'AH0', 'N', 'AW2', 'T']]\n",
    "}\n",
    "cmu_dict.update(custom_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce299bc1-344a-4356-835c-aceaa4234196",
   "metadata": {},
   "source": [
    "Then, I thought it would be useful to create a simple function that gives a word's arpabet transcription using CMUdict. That way, I can just use this function for all my transcriptions.\n",
    "\n",
    "Something to note here: sometimes, CMUdict will have multiple transcriptions of a single word. For simplicity in these examples and tests, I'm defaulting to the first tramscription in CMUdict. It would be nice to make this function a bit more complex, as well as the following functions, and potentially account for all the variants of a certain word's pronunciation, in case that matches better across words, especially for substitutions. \n",
    "\n",
    "Since we are looking at the results of ASR processing and not actual speech here, I did not think it was necessary to include the variants. So, as of now, CMUdict will default to the first variant of its tramscriptions for both the prompt's words and the transcript's words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7471ac58-3bf7-4fa7-84eb-b685f727b536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_arpabet(word):\n",
    "    #make the word lowercase because CMUdict only uses lowercase\n",
    "    word_lower = word.lower()  \n",
    "    if word_lower in cmu_dict:\n",
    "        return (cmu_dict[word_lower])[0]\n",
    "    else:\n",
    "        return \"NOT FOUND\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c11e5d-ae0f-4216-a072-0665dfb01344",
   "metadata": {},
   "source": [
    "Next, we will follow a similar process to the word error measurement. First of all, the input for this function will only include the problem words from the word error measurement, instead of iterating through the arpabet of the entire transcription again. Specifically, the input includes 2 lists: the first is the list of words in the prompt missing from the tramscript, and second is the list of words in the transcript that are not in the prompt.\n",
    "\n",
    "This function uses the function defined above to give each word's arpabet transcription in a list. Since our goal is just to compare all the remaining phonemes, I store all the arpabet characters into one list each for the prompt and the transcript. This can also potentially be refined, depending on the patterns of errors I get with edge cases. For my current examples, this method seems to work well with my intuitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3dbf7f8-8478-4398-a735-70da36d51415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arpabet_list_compare(target_list, error_list):\n",
    "    target_arpabet = []\n",
    "    for i in target_list:\n",
    "        arpabet_transcription = word_to_arpabet(i)\n",
    "        for j in arpabet_transcription:\n",
    "                target_arpabet.append(j)\n",
    "        \n",
    "    error_arpabet = []\n",
    "    for i in error_list:\n",
    "        arpabet_transcription = word_to_arpabet(i)\n",
    "        for j in arpabet_transcription:\n",
    "                error_arpabet.append(j)\n",
    "    print(\"arpabet versions of words: \", [target_arpabet, error_arpabet])\n",
    "    return [target_arpabet, error_arpabet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69216d6d-0cd7-45ad-8e52-00ca23288315",
   "metadata": {},
   "source": [
    "Again, in a similar process to the word level errors, I add extra empty strings to the smaller list for easier iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20318efc-91f2-43cd-8aa8-937a833e5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equalize_arpabet_lengths(targ_arpabet, err_arpabet):\n",
    "    if len(err_arpabet) > len(targ_arpabet):\n",
    "        extra_errors = len(err_arpabet) - len(targ_arpabet)\n",
    "        for i in range(0, extra_errors):\n",
    "            targ_arpabet.append(\"\")\n",
    "            \n",
    "    elif len(targ_arpabet) > len(err_arpabet):\n",
    "        targ_missed = len(targ_arpabet) - len(err_arpabet)\n",
    "        for i in range(0, targ_missed):\n",
    "            err_arpabet.append(\"\")\n",
    "\n",
    "    return [targ_arpabet, err_arpabet]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3217c929-50b5-489b-825a-f2a0c3795d5a",
   "metadata": {},
   "source": [
    "This returns 2 lists with the same number of elements. Now, I use the same skeleton of the function to find errors for words, but with the arpabet lists instead. This function also returns 2 lists, the first with characters in the transcription but not in the prompt,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58bf35b0-d3ac-47cb-a3e8-c94141af39be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_words(ready_targ_arpa, ready_err_arpa):\n",
    "\n",
    "    #this list stores all unclaimed characters from the prompt\n",
    "    targ_arpa_leftovers = []\n",
    "    for i in ready_targ_arpa:\n",
    "        if (i in ready_err_arpa and i in ready_targ_arpa):\n",
    "            ready_err_arpa.remove(i)\n",
    "        else:\n",
    "            targ_arpa_leftovers += [i]\n",
    "\n",
    "    #this removes empty strings and gives us the final characters from the prompt that did not appear in the transcript\n",
    "    ready_targ_arpa = []\n",
    "    for i in targ_arpa_leftovers:\n",
    "        if len(i) != 0:\n",
    "            ready_targ_arpa.append(i)\n",
    "\n",
    "    #this gives us the characters in the transcription that were not found in the prompt\n",
    "    temp_err_arpa_list = []\n",
    "    for i in ready_err_arpa:\n",
    "        if len(i) != 0:\n",
    "            temp_err_arpa_list.append(i)\n",
    "    ready_err_arpa = temp_err_arpa_list\n",
    "\n",
    "    return [ready_targ_arpa, ready_err_arpa]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1393a85-d2ee-4f23-9eee-2ebb031a1cc7",
   "metadata": {},
   "source": [
    "To count errors, I can use the same counter function defined for the word error measurements. So, my next step now is to compile all the functions together and get the arpabet error differences for 2 lists of error words.\n",
    "\n",
    "The output for this final function is a list of 3 elements, the first being the characters in the prompt that are missing from the transcript, the second being the characters from the transcript that are not in the prompt, and the third being the number of errors. The number of errors is measured, once again, using whichever list has a greater number of characters, since that tends to account for substitutions and additions without double-counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fa66fb94-82a7-4af6-baee-03f69d5be3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arpa_error_finder(target_list, error_list):\n",
    "    #step 1\n",
    "    arpa_compare_result = arpabet_list_compare(target_list, error_list)\n",
    "    targ_arpabet = arpa_compare_result[0]\n",
    "    err_arpabet = arpa_compare_result[1]\n",
    "\n",
    "    #step 2\n",
    "    eq_arpabet_lists = equalize_arpabet_lengths(targ_arpabet, err_arpabet)\n",
    "    ready_targ_arpa = eq_arpabet_lists[0]\n",
    "    ready_err_arpa = eq_arpabet_lists[1]\n",
    "\n",
    "    #step 3\n",
    "    final_error_lists = get_error_words(ready_targ_arpa, ready_err_arpa)\n",
    "    final_targ_list = final_error_lists[0]\n",
    "    final_err_list = final_error_lists[1]\n",
    "\n",
    "    #step 4\n",
    "    final_err_count = counter(final_targ_list, final_err_list)\n",
    "\n",
    "    return [final_targ_list, final_err_list, final_err_count]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6974b383-4aaa-488b-b49f-bba017fbd1a8",
   "metadata": {},
   "source": [
    "### Testing Phoneme Error Measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed85518d-de9b-46f8-97ff-4240c99e73cb",
   "metadata": {},
   "source": [
    "Now, I will use the results from the word-level error measurement to test the phoneme-level errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51e09931-b8aa-49a8-bb26-0c171be42c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arpabet 1\n",
      "List of words missing from prompt in transcription:  ['bangalore']\n",
      "List of words in transcription not present in prompt:  ['bong', 'load']\n",
      "arpabet versions of words:  [['B', 'AE1', 'NG', 'G', 'AH0', 'L', 'AO2', 'R'], ['B', 'AA1', 'NG', 'L', 'OW1', 'D']]\n",
      "[['AE1', 'G', 'AH0', 'AO2', 'R'], ['AA1', 'OW1', 'D'], 5]\n",
      "\n",
      "Arpabet 2\n",
      "List of words missing from prompt in transcription:  ['for', 'bangalore']\n",
      "List of words in transcription not present in prompt:  []\n",
      "arpabet versions of words:  [['F', 'AO1', 'R', 'B', 'AE1', 'NG', 'G', 'AH0', 'L', 'AO2', 'R'], []]\n",
      "[['F', 'AO1', 'R', 'B', 'AE1', 'NG', 'G', 'AH0', 'L', 'AO2', 'R'], [], 11]\n",
      "\n",
      "Arpabet 3\n",
      "List of words missing from prompt in transcription:  ['leaving']\n",
      "List of words in transcription not present in prompt:  ['leafing', 'then']\n",
      "arpabet versions of words:  [['L', 'IY1', 'V', 'IH0', 'NG'], ['L', 'IY1', 'F', 'IH0', 'NG', 'DH', 'EH1', 'N']]\n",
      "[['V'], ['F', 'DH', 'EH1', 'N'], 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using the first set of examples\n",
    "target_sentence_1 = \"She was leaving for Bangalore that day.\"\n",
    "error_sentence_1 = \"She was leaving for bong load that day.\"\n",
    "error_sentence_2 = \"She was leaving that day.\"\n",
    "error_sentence_3 = \"She was leafing for Bangalore that day then.\"\n",
    "\n",
    "print(\"Arpabet 1\")\n",
    "err1_results = error_finder(target_sentence_1, error_sentence_1)\n",
    "target_list = err1_results[0]\n",
    "error_list = err1_results[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"Arpabet 2\")\n",
    "err2_results = error_finder(target_sentence_1, error_sentence_2)\n",
    "target_list = err2_results[0]\n",
    "error_list = err2_results[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"Arpabet 3\")\n",
    "err3_results = error_finder(target_sentence_1, error_sentence_3)\n",
    "target_list = err3_results[0]\n",
    "error_list = err3_results[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccb2db-be3e-4571-8cd0-e1525711299f",
   "metadata": {},
   "source": [
    "As seen from these results, the phonemic transcriptions give us much more nuanced insight into where exactly the transcriptions are missing out or substituting phonemes. The error counts are also surprisingly intuitive and easy to understand. Using these phonemic error measurements gives me a better understanding of the level of differences between each of the transcripts. \n",
    "\n",
    "To continue testing, I'll bring down the previous examples again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "563c08f9-4e52-4bc2-a2ef-5ac46e08f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You haven't even been to the In-n-Out in the Outback Steakhouse neighborhood?\n",
      "\n",
      "socal_1_whisper\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['end', 'and', 'out']\n",
      "arpabet versions of words:  [['IH2', 'N', 'AH0', 'N', 'AW2', 'T'], ['EH1', 'N', 'D', 'AH0', 'N', 'D', 'AW1', 'T']]\n",
      "[['IH2', 'AW2'], ['EH1', 'D', 'D', 'AW1'], 4]\n",
      "\n",
      "indian_1_whisper\n",
      "List of words missing from prompt in transcription:  ['innout', 'steakhouse', 'neighborhood']\n",
      "List of words in transcription not present in prompt:  ['inn', 'and', 'out', 'stack', 'or', 'sniper', 'herd']\n",
      "arpabet versions of words:  [['IH2', 'N', 'AH0', 'N', 'AW2', 'T', 'S', 'T', 'EY1', 'K', 'HH', 'AW2', 'S', 'N', 'EY1', 'B', 'ER0', 'HH', 'UH2', 'D'], ['IH1', 'N', 'AH0', 'N', 'D', 'AW1', 'T', 'S', 'T', 'AE1', 'K', 'AO1', 'R', 'S', 'N', 'AY1', 'P', 'ER0', 'HH', 'ER1', 'D']]\n",
      "[['IH2', 'AW2', 'EY1', 'AW2', 'EY1', 'B', 'HH', 'UH2'], ['IH1', 'AW1', 'AE1', 'AO1', 'R', 'AY1', 'P', 'ER1', 'D'], 9]\n",
      "\n",
      "indian_2_whisper\n",
      "List of words missing from prompt in transcription:  ['you', 'innout']\n",
      "List of words in transcription not present in prompt:  ['youll', 'inn', 'and', 'out']\n",
      "arpabet versions of words:  [['Y', 'UW1', 'IH2', 'N', 'AH0', 'N', 'AW2', 'T'], ['N', 'O', 'T', ' ', 'F', 'O', 'U', 'N', 'D', 'IH1', 'N', 'AH0', 'N', 'D', 'AW1', 'T']]\n",
      "[['Y', 'UW1', 'IH2', 'AW2'], ['O', ' ', 'F', 'O', 'U', 'D', 'IH1', 'N', 'N', 'D', 'AW1', 'T'], 12]\n",
      "\n",
      "socal_1_azure\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['and', 'out', 'in']\n",
      "arpabet versions of words:  [['IH2', 'N', 'AH0', 'N', 'AW2', 'T'], ['AH0', 'N', 'D', 'AW1', 'T', 'IH0', 'N']]\n",
      "[['IH2', 'AW2'], ['D', 'AW1', 'IH0'], 3]\n",
      "\n",
      "indian_1_azure\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['and', 'out', 'in']\n",
      "arpabet versions of words:  [['IH2', 'N', 'AH0', 'N', 'AW2', 'T'], ['AH0', 'N', 'D', 'AW1', 'T', 'IH0', 'N']]\n",
      "[['IH2', 'AW2'], ['D', 'AW1', 'IH0'], 3]\n",
      "\n",
      "indian_2_azure\n",
      "List of words missing from prompt in transcription:  ['innout']\n",
      "List of words in transcription not present in prompt:  ['and', 'out', 'in']\n",
      "arpabet versions of words:  [['IH2', 'N', 'AH0', 'N', 'AW2', 'T'], ['AH0', 'N', 'D', 'AW1', 'T', 'IH0', 'N']]\n",
      "[['IH2', 'AW2'], ['D', 'AW1', 'IH0'], 3]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_2 = \"You haven't even been to the In-n-Out in the Outback Steakhouse neighborhood?\"\n",
    "print(target_sentence_2)\n",
    "print()\n",
    "\n",
    "socal_1_whisper = \"You haven't even been to the end and out in the Outback Steakhouse neighborhood.\"\n",
    "indian_1_whisper = \"You haven't even been to the inn and out in the outback stack or sniper herd.\"\n",
    "indian_2_whisper = \"You'll haven't even been to the inn and out in the Outback Steakhouse neighborhood.\"\n",
    "\n",
    "socal_1_azure = \"you haven't even been to the in and out in the outback steakhouse neighborhood\"\n",
    "indian_1_azure = \"you haven't even been to the in and out in the outback steakhouse neighborhood\"\n",
    "indian_2_azure = \"you haven't even been to the in and out in the outback steakhouse neighborhood\"\n",
    "\n",
    "print(\"socal_1_whisper\")\n",
    "for_arpa = error_finder(target_sentence_2, socal_1_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_whisper\")\n",
    "for_arpa = error_finder(target_sentence_2, indian_1_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_whisper\")\n",
    "for_arpa = error_finder(target_sentence_2, indian_2_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"socal_1_azure\")\n",
    "for_arpa = error_finder(target_sentence_2, socal_1_azure)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_azure\")\n",
    "for_arpa = error_finder(target_sentence_2, indian_1_azure)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_azure\")\n",
    "for_arpa = error_finder(target_sentence_2, indian_2_azure)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6c18d3-e88e-4ba2-ac17-d728e8af56b4",
   "metadata": {},
   "source": [
    "This is really great! It gives me a more nuanced understanding of where exactly Whisper is messing up, and the difference between the Southern California dialect and the Indian dialects is pretty big in this case.\n",
    "\n",
    "Microsoft Azure did pretty great with this one, so the next example will help me see the effects of phoneme-level errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e970b269-c951-4a68-81b6-661ee841c061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bro sure with all my work I've made it to the brochure.\n",
      "\n",
      "socal_1_whisper\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure']\n",
      "List of words in transcription not present in prompt:  ['brochure']\n",
      "arpabet versions of words:  [['B', 'R', 'OW1', 'SH', 'UH1', 'R'], ['B', 'R', 'OW0', 'SH', 'UH1', 'R']]\n",
      "[['OW1'], ['OW0'], 1]\n",
      "\n",
      "indian_1_whisper\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure', 'with', 'it', 'brochure']\n",
      "List of words in transcription not present in prompt:  ['im', 'going', 'show', 'you', 'to', 'brush']\n",
      "arpabet versions of words:  [['B', 'R', 'OW1', 'SH', 'UH1', 'R', 'W', 'IH1', 'DH', 'IH1', 'T', 'B', 'R', 'OW0', 'SH', 'UH1', 'R'], ['IH1', 'M', 'G', 'OW1', 'IH0', 'NG', 'SH', 'OW1', 'Y', 'UW1', 'T', 'UW1', 'B', 'R', 'AH1', 'SH']]\n",
      "[['UH1', 'R', 'W', 'DH', 'IH1', 'B', 'R', 'OW0', 'UH1', 'R'], ['M', 'G', 'IH0', 'NG', 'OW1', 'Y', 'UW1', 'UW1', 'AH1'], 10]\n",
      "\n",
      "indian_2_whisper\n",
      "List of words missing from prompt in transcription:  ['sure']\n",
      "List of words in transcription not present in prompt:  ['share']\n",
      "arpabet versions of words:  [['SH', 'UH1', 'R'], ['SH', 'EH1', 'R']]\n",
      "[['UH1'], ['EH1'], 1]\n",
      "\n",
      "socal_1_azure\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure']\n",
      "List of words in transcription not present in prompt:  ['brochure']\n",
      "arpabet versions of words:  [['B', 'R', 'OW1', 'SH', 'UH1', 'R'], ['B', 'R', 'OW0', 'SH', 'UH1', 'R']]\n",
      "[['OW1'], ['OW0'], 1]\n",
      "\n",
      "indian_1_azure\n",
      "List of words missing from prompt in transcription:  ['bro', 'sure']\n",
      "List of words in transcription not present in prompt:  ['share']\n",
      "arpabet versions of words:  [['B', 'R', 'OW1', 'SH', 'UH1', 'R'], ['SH', 'EH1', 'R']]\n",
      "[['B', 'OW1', 'UH1', 'R'], ['EH1'], 4]\n",
      "\n",
      "indian_2_azure\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "arpabet versions of words:  [[], []]\n",
      "[[], [], 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_3 = \"Bro sure with all my work I've made it to the brochure.\"\n",
    "print(target_sentence_3)\n",
    "print()\n",
    "\n",
    "indian_2_whisper = \"Bro, share with all my work. I've made it to the brochure.\"\n",
    "socal_1_whisper = \"brochure with all my work I've made it to the brochure.\"\n",
    "indian_1_whisper = \"I'm going to show you all my work I've made to the brush.\"\n",
    "\n",
    "indian_2_azure = \"bro sure with all my work i've made it to the brochure\"\n",
    "socal_1_azure = \"brochure with all my work i've made it to the brochure\"\n",
    "indian_1_azure = \"share with all my work i've made it to the brochure\"\n",
    "\n",
    "print(\"socal_1_whisper\")\n",
    "for_arpa = error_finder(target_sentence_3, socal_1_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_whisper\")\n",
    "for_arpa = error_finder(target_sentence_3, indian_1_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_whisper\")\n",
    "for_arpa = error_finder(target_sentence_3, indian_2_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"socal_1_azure\")\n",
    "for_arpa = error_finder(target_sentence_3, socal_1_azure)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_azure\")\n",
    "for_arpa = error_finder(target_sentence_3, indian_1_azure)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_azure\")\n",
    "for_arpa = error_finder(target_sentence_3, indian_2_azure)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274676c4-4c71-41c5-9b15-176f82b80b7a",
   "metadata": {},
   "source": [
    "Whisper's struggle with the Indian dialects becomes starkly visible using this transcription. Interestingly, though, this is a good way to see that Azure is also doing pretty poorly with one of the Indian dialects compared to the other one and the Souther California dialect. This was not a clear distinction at the word-level, but becomes visible and easily quantifiable at the phoneme-level.\n",
    "\n",
    "There seem to be some great advantages to computing the phoneme-level errors for measurement here. I will use the last set of examples to see if Whisper's error rates continue to be more starkly bad for Indian dialects. We know that Azure got all of this right, so we're going to skip over those transcriptions and focus on Whisper specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f64ae594-6581-49bc-b9c8-4b39816fee0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is leaving for Bengaluru tomorrow.\n",
      "\n",
      "socal_1_whisper\n",
      "List of words missing from prompt in transcription:  []\n",
      "List of words in transcription not present in prompt:  []\n",
      "arpabet versions of words:  [[], []]\n",
      "[[], [], 0]\n",
      "\n",
      "indian_1_whisper\n",
      "List of words missing from prompt in transcription:  ['he', 'is', 'bengaluru']\n",
      "List of words in transcription not present in prompt:  ['hes', 'bangalore']\n",
      "arpabet versions of words:  [['HH', 'IY1', 'IH1', 'Z', 'B', 'EH', 'NG', 'G', 'UH', 'L', 'UW', 'R', 'UW'], ['N', 'O', 'T', ' ', 'F', 'O', 'U', 'N', 'D', 'B', 'AE1', 'NG', 'G', 'AH0', 'L', 'AO2', 'R']]\n",
      "[['HH', 'IY1', 'IH1', 'Z', 'EH', 'UH', 'UW', 'UW'], ['N', 'O', 'T', ' ', 'F', 'O', 'U', 'N', 'D', 'AE1', 'AH0', 'AO2'], 12]\n",
      "\n",
      "indian_2_whisper\n",
      "List of words missing from prompt in transcription:  ['bengaluru']\n",
      "List of words in transcription not present in prompt:  ['bangalore']\n",
      "arpabet versions of words:  [['B', 'EH', 'NG', 'G', 'UH', 'L', 'UW', 'R', 'UW'], ['B', 'AE1', 'NG', 'G', 'AH0', 'L', 'AO2', 'R']]\n",
      "[['EH', 'UH', 'UW', 'UW'], ['AE1', 'AH0', 'AO2'], 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_sentence_4 = \"He is leaving for Bengaluru tomorrow.\"\n",
    "print(target_sentence_4)\n",
    "print()\n",
    "\n",
    "indian_1_whisper = \"He's leaving for Bangalore tomorrow\"\n",
    "indian_2_whisper = \"He is leaving for Bangalore tomorrow.\"\n",
    "socal_1_whisper = \"He is leaving for Bengaluru tomorrow.\"\n",
    "\n",
    "print(\"socal_1_whisper\")\n",
    "for_arpa = error_finder(target_sentence_4, socal_1_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_1_whisper\")\n",
    "for_arpa = error_finder(target_sentence_4, indian_1_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()\n",
    "\n",
    "print(\"indian_2_whisper\")\n",
    "for_arpa = error_finder(target_sentence_4, indian_2_whisper)\n",
    "target_list = for_arpa[0]\n",
    "error_list = for_arpa[1]\n",
    "print(arpa_error_finder(target_list, error_list))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7576898-f7cd-4f60-b976-302c4ebb934e",
   "metadata": {},
   "source": [
    "While Whisper does perfectly with the Southern California dialect and not-so-bad with one of the Indian dialects, how much worse it does with the other one is a lot clearer using phonemic analysis rather than word-level analysis.\n",
    "\n",
    "The magnitude of differences is fascinating and very insightful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7762986-4b50-49b0-a1f1-7916753538fb",
   "metadata": {},
   "source": [
    "### Overall Phoneme Error Measurement: Surprisingly Intuitive, Very Insightful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb31a5-da0c-43a3-a8c1-ee3b205e6681",
   "metadata": {},
   "source": [
    "Although I had initially hypothesized that word-level error measurement would be most intuitive and useful, I found that phoneme-level analysis was a lot more informative and helpful to my analysis. \n",
    "\n",
    "Calculating errors using CMUdict is a great way to approximate the magnitudes of differences across different ASR services and dialects. The granularity from phoneme-level error calculations turned out to be extremely interesting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0884c3-3475-448e-86b0-fce0c5cf9d4c",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc56e910-79a1-4f0c-a9d7-014fa8e4b491",
   "metadata": {},
   "source": [
    "Overall, this was a really interesting project! I was able to use my knowledge from class and gain some meaningful insights into how ASR works, and why it's making the kinds of errors it is.\n",
    "\n",
    "My big learnings from this project were:\n",
    "1) Several factors matter when recording the audio itself. The speaker's speech style can be a really important factor in how ASR processes the speech. From discussions in class, this may be due to the training data for these systems using formal speech on average, biases in the training data, or something else entirely. It was really interesting to try using different levels of background noise and varying speech styles to see how results compared across them. The homeworks from class got me thinking about this - I will keep an eye out for it now when I'm recording participants or hearing their audio files!\n",
    "2) Differences in accuracy across dialects varies a lot by service: I was not expecting Microsoft Azure to do nearly as well as it did! I was pleasantly surprised by Azure's results across dialects being fairly consistent, whereas Whisper was visibly worse (in both error measurements) with Indian dialects. I had expected the variation to be more consistent across different ASR services, but the training data being regularly updated and getting unique dialects might've given Azure the edge here.\n",
    "3) Phoneme-Level Error Measurements are great! I discussed this already, but it's really interestnig how much of a difference it can make to look closely at these transcripts. It's also surprisingly challenging to find a good way to do this - I still have several kinks I want to work out in calculating the phoneme-level errors that I mentioned throughout.\n",
    "4) Overall, Southern California dialects are still transcribed better: In spite of Azure's impressive performance, the phoneme-level analysis showed that generally, Southern California dialects are still transcribed better. This is probably due to this dialect being more fully represented in the training datasets for these systems.\n",
    "\n",
    "This project has led to many ethical questions for those working on and creating these ASR systems. I think there's a lot of room to grow and explore here, and I'm glad I could use this project as an opportunity to figure some part of it out. Maybe, just maybe, more equitable training data is the solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aa477e-2d8e-489a-b935-ffbd012daa33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
